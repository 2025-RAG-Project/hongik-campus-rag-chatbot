"""
í™ìµëŒ€í•™êµ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ì½”ë“œ (ìµœì†Œ ì˜ì¡´ì„± ë²„ì „)
- 2ë²ˆ: í•™ì‚¬ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒ í¬ë¡¤ë§ (ê° ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ê¹Œì§€ ë“¤ì–´ê°€ì„œ ë‚´ìš©/ì²¨ë¶€ ì²˜ë¦¬)
- 3ë²ˆ: ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ ê°œì„¤ê³¼ëª© (JS ë¡œë”© â†’ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì•Œì•„ì•¼ í•´ì„œ TODOë¡œ ì²˜ë¦¬)
- 4ë²ˆ: ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ í•™ê³¼ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (ê° ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ê¹Œì§€)
"""

from unicodedata import name
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import time
import urllib3
from urllib.parse import urljoin, urlparse, parse_qs
import PyPDF2
from io import BytesIO
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class HongikCrawler:
    DATE_PATTERN = re.compile(r"\d{4}\.\d{2}\.\d{2}")
    ATTACH_EXTS = (".pdf", ".hwp", ".hwpx", ".doc", ".docx",
                   ".xls", ".xlsx", ".ppt", ".pptx", ".zip")

    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko)"
            )
        }

    # ---------------- ê³µí†µ ìœ í‹¸ ---------------- #

    def _extract_date_from_row(self, tr):
        """tr ì•ˆì˜ td í…ìŠ¤íŠ¸ ì¤‘ 'YYYY.MM.DD' í˜•ì‹ì„ ì°¾ì•„ datetimeìœ¼ë¡œ ë°˜í™˜"""
        for td in tr.find_all("td"):
            text = td.get_text(strip=True)
            m = self.DATE_PATTERN.search(text)
            if m:
                try:
                    return datetime.strptime(m.group(), "%Y.%m.%d")
                except ValueError:
                    return None
        return None

    def _extract_article_text(self, soup, title=None):
        """
        ê²Œì‹œë¬¼ ìƒì„¸ í˜ì´ì§€ì—ì„œ íƒ€ì´í‹€/ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        - title: ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ì œëª©(ìˆìœ¼ë©´ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë³¸ë¬¸ êµ¬ê°„ ì˜ë¼ëƒ„)
        - í´ë˜ìŠ¤ëª…(.view_title ë“±)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¼ë‚´ëŠ” ë°©ì‹
        """
        full_text = soup.get_text("\n", strip=True)
        lines = [l.strip() for l in full_text.splitlines() if l.strip()]

        # ì œëª© ìœ„ì¹˜ ì°¾ê¸° (ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ì œëª©ì´ ì‹¤ì œ í˜ì´ì§€ì—ë„ ë™ì¼í•˜ê²Œ ë‚˜íƒ€ë‚¨)
        title_line = None
        idx_title = 0
        if title:
            for i, line in enumerate(lines):
                if line == title:
                    title_line = line
                    idx_title = i
                    break

        if not title_line:
            # íƒ€ì´í‹€ì„ ëª» ì°¾ìœ¼ë©´ ì²« ë²ˆì§¸ë¡œ 'ê³µì§€ì‚¬í•­' ì•„ë˜ ë‚˜ì˜¤ëŠ” ì¤„ì„ ì œëª©ì´ë¼ê³  ê°€ì •
            # (ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ì™„ì „ ì •í™•í•˜ì§„ ì•Šì§€ë§Œ ìµœì†Œ ë™ì‘ìš©)
            for i, line in enumerate(lines):
                if "ê³µì§€ì‚¬í•­" in line:
                    # ê·¸ ë‹¤ìŒ non-empty lineì„ ì œëª©ìœ¼ë¡œ
                    for j in range(i + 1, len(lines)):
                        if lines[j]:
                            title_line = lines[j]
                            idx_title = j
                            break
                    break

        if not title_line:
            # ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´ ê·¸ëƒ¥ ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ ì²˜ë¦¬
            title_line = lines[0] if lines else ""
            idx_title = 0

        # ë³¸ë¬¸ êµ¬ê°„: [ì œëª© ë‹¤ìŒ ì¤„ ~ 'ì´ì „ê¸€/ë‹¤ìŒê¸€/ëª©ë¡' ì „ê¹Œì§€]
        body_lines = []
        for line in lines[idx_title + 1 :]:
            if line in ("ì´ì „ê¸€", "ë‹¤ìŒê¸€", "ëª©ë¡"):
                break
            # ê³µìœ /í”„ë¦°í„°/ë©”ë‰´ ê°™ì€ ë©”íƒ€ í…ìŠ¤íŠ¸ ì œê±°
            if any(
                key in line
                for key in ("ì¹´ì¹´ì˜¤ ê³µìœ í•˜ê¸°", "í˜ì´ìŠ¤ë¶ ê³µìœ í•˜ê¸°", "URL ê³µìœ í•˜ê¸°", "í”„ë¦°í„°")
            ):
                continue
            # 'ì²¨ë¶€íŒŒì¼' ë¬¸êµ¬ ìì²´ëŠ” ì œì™¸ (íŒŒì¼ ë¦¬ìŠ¤íŠ¸ëŠ” ë”°ë¡œ ì²˜ë¦¬)
            if "ì²¨ë¶€íŒŒì¼" in line:
                continue
            body_lines.append(line)

        body_text = "\n".join(body_lines).strip()
        return title_line, body_text

    def _extract_attachments(self, soup, page_url):
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        - ì´ë¦„ê³¼ URL, (PDFì¸ ê²½ìš° ë‚´ìš© í…ìŠ¤íŠ¸)ê¹Œì§€
        - .pdf ì™¸ í™•ì¥ìëŠ” URLë§Œ ì €ì¥ (hwp í•´ì„ì€ ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•´ì„œ ì—¬ê¸°ì„  ì œì™¸)
        """
        attachments = []
        for a in soup.find_all("a"):
            name = a.get_text(strip=True)
            if not name:
                continue
            lower = name.lower()
            if not lower.endswith(self.ATTACH_EXTS):
                continue

            href = a.get("href")
            if not href or href.startswith("javascript"):
                continue

            file_url = urljoin(page_url, href)
            attach = {"name": name, "url": file_url, "content": None}

            # # PDFì¸ ê²½ìš°ë§Œ ë‚´ìš©ê¹Œì§€ ì¶”ì¶œ (PyPDF2 ì‚¬ìš©)
            # if lower.endswith(".pdf"):
            #     try:
            #         resp = self.session.get(file_url, headers=self.headers)
            #         if resp.ok:
            #             attach["content"] = self.extract_pdf_text(resp.content)
            #     except Exception:
            #         attach["content"] = "PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"

            attachments.append(attach)

        return attachments

    def extract_pdf_text(self, pdf_bytes):
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_file = BytesIO(pdf_bytes)
            reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in reader.pages:
                # PyPDF2ì˜ extract_textëŠ” ë²„ì „ì— ë”°ë¼ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                text += page.extract_text() or ""
            return text.strip() or "PDFì— ì¶”ì¶œ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        except Exception:
            return "PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"

    def _fetch_detail(self, post_url, title, post_date):
        try:
            resp = self.session.get(
                post_url,
                headers=self.headers,
                timeout=5
            )
            resp.raise_for_status()
        except Exception as e:
            print(f"[ì—ëŸ¬] ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨(HTTP): {post_url}, error={e}")
            return None

        try:
            detail_soup = BeautifulSoup(resp.text, "lxml")

            real_title, body = self._extract_article_text(detail_soup, title)
            attachments = self._extract_attachments(detail_soup, post_url)
        except Exception as e:
            print(f"[ì—ëŸ¬] ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨: {post_url}, error={e}")
            real_title = title
            body = ""
            attachments = []

        if hasattr(post_date, "strftime"):
            date_str = post_date.strftime("%Y.%m.%d")
        else:
            date_str = str(post_date)

        return {
            "url": post_url,
            "title": real_title,
            "content": body,
            "date": date_str,
            "attachments": attachments,
        }



    # ---------------- 1. CN í™ìµ ë¡œê·¸ì¸ ---------------- #

    def login_cn_hongik(self, user_id, password):
        """
        1ë²ˆ ë§í¬ - CN í™ìµ ë¡œê·¸ì¸
        âš ï¸ ì£¼ì˜: ì´ í˜ì´ì§€ëŠ” ì‹¤ì œë¡œëŠ” SSO/JS ê¸°ë°˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.
        â†’ ë°˜ë“œì‹œ ë¸Œë¼ìš°ì € ê°œë°œìë„êµ¬(Network íƒ­)ë¡œ ì‹¤ì œ ë¡œê·¸ì¸ ìš”ì²­ URL/íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•´ì„œ
          ì•„ë˜ login_action_url / login_dataë¥¼ ìˆ˜ì •í•´ì•¼ í•œë‹¤.
        """
        login_page = "https://cn.hongik.ac.kr/stud/"

        # ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì† (ì¿ í‚¤ ì„¸íŒ… ìš©ë„)
        try:
            self.session.get(login_page, headers=self.headers, timeout=10)
        except Exception:
            return False

        # TODO: ê°œë°œìë„êµ¬ì—ì„œ ì‹¤ì œ ë¡œê·¸ì¸ ìš”ì²­ URL/í•„ë“œ í™•ì¸ í›„ ìˆ˜ì •
        login_action_url = login_page  # ì˜ˆì‹œ: "https://cn.hongik.ac.kr/stud/jsp/login/check_login.jsp"
        login_data = {
            "id": user_id,
            "pw": password,
            # ì‹¤ì œ í•„ë“œëª…ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        }

        try:
            resp = self.session.post(
                login_action_url, data=login_data, headers=self.headers, timeout=10
            )
            return resp.ok
        except Exception:
            return False

    # ---------------- 2. í•™ì‚¬ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒ ---------------- #


    def _crawl_single_board(self, base_url, from_date, to_date=None):
        """
        - base_urlë¶€í„° ì‹œì‘í•´ì„œ
        - b-paging-wrap ì•ˆì˜ 'ë‹¤ìŒ í˜ì´ì§€' ë§í¬ë¥¼ íƒ€ê³  ê³„ì† ë‚´ë ¤ê°€ë©´ì„œ
        - from_date ~ to_date ì‚¬ì´ì˜ ê¸€ë“¤ë§Œ ìƒì„¸ í¬ë¡¤ë§í•´ì„œ itemì„ yield
        """

        current_url = base_url
        visited = set()

        while True:
            # 0) ë¬´í•œë£¨í”„ ë°©ì§€
            if current_url in visited:
                print(f"[ì¤‘ë‹¨] ì´ë¯¸ ë°©ë¬¸í•œ URL ì¬ë°©ë¬¸ ê°ì§€: {current_url}")
                break
            visited.add(current_url)

            # 1) ëª©ë¡ í˜ì´ì§€ ìš”ì²­
            try:
                resp = self.session.get(
                    current_url,
                    headers=self.headers,
                    timeout=5
                )
            except Exception as e:
                print(f"[ì—ëŸ¬] ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {current_url}, error={e}")
                break

            if not resp.ok:
                print(f"[ì—ëŸ¬] ëª©ë¡ í˜ì´ì§€ status={resp.status_code}: {current_url}")
                break

            # 2) íŒŒì‹±
            soup = BeautifulSoup(resp.text, "lxml")
            rows = soup.select("tbody tr") or soup.select("tr")

            page_has_not_too_old_post = False   # from_date ì´ìƒ ê¸€ì´ ìˆì—ˆëŠ”ì§€
            tasks = []                          # (post_url, title, post_date, post_no)

            for tr in rows:
                link_elem = tr.find("a")
                if not link_elem:
                    continue

                # ê²Œì‹œê¸€ ë²ˆí˜¸ (ìˆìœ¼ë©´)
                num_td = tr.find("td", class_="b-num-box")
                post_no = None
                if num_td:
                    try:
                        post_no = int(num_td.get_text(strip=True))
                    except ValueError:
                        post_no = None

                # ë‚ ì§œ ì¶”ì¶œ
                post_date = self._extract_date_from_row(tr)
                if not post_date:
                    continue

                # post_date íƒ€ì… í†µì¼ (datetime â†’ date, strì´ë©´ íŒŒì‹±)
                if isinstance(post_date, datetime):
                    post_date = post_date.date()
                elif isinstance(post_date, str):
                    # ì˜ˆ: "2025.11.29" í˜•íƒœë¼ë©´
                    try:
                        post_date = datetime.strptime(post_date, "%Y.%m.%d").date()
                    except ValueError:
                        # í˜•ì‹ì´ ë‹¤ë¥´ë©´ ê·¸ëƒ¥ ê±´ë„ˆë›´ë‹¤
                        continue

                # â‘  from_dateë³´ë‹¤ ì˜›ë‚ ì´ë©´: ì´ë²ˆ ë²”ìœ„ì—ì„œëŠ” í•„ìš” ì—†ìŒ
                if post_date < from_date:
                    # ì´ ê¸€ì€ ë„ˆë¬´ ì˜¤ë˜ëœ ê¸€ â†’ ì´ë²ˆ ë²”ìœ„ì—ì„œëŠ” ìŠ¤í‚µ
                    continue

                # ì—¬ê¸°ê¹Œì§€ ì™”ìœ¼ë©´ ì ì–´ë„ from_date ì´ìƒì¸ ê¸€ì´ë¼ëŠ” ëœ»
                page_has_not_too_old_post = True

                # â‘¡ to_dateê°€ ìˆê³ , ê·¸ë³´ë‹¤ ë” "ìµœê·¼"ì´ë©´: ì´ë²ˆ ë²”ìœ„ê°€ ì•„ë‹ˆë¼ ë” ìµœì‹ ë²”ìœ„ì—ì„œ ë‹¤ë£° ëŒ€ìƒ
                if to_date is not None and post_date > to_date:
                    continue

                # â‘¢ ì‹¤ì œ ìˆ˜ì§‘ ëŒ€ìƒ (from_date ~ to_date ì‚¬ì´)
                title = link_elem.get_text(strip=True)
                href = link_elem.get("href")
                if not href:
                    continue

                post_url = urljoin(current_url, href)
                tasks.append((post_url, title, post_date, post_no))

            # ë””ë²„ê¹…ìš©: ì´ í˜ì´ì§€ ìš”ì•½
            print(
                f"[ëª©ë¡] url={current_url}, rows={len(rows)}, "
                f"from_dateì´ìƒì¡´ì¬={page_has_not_too_old_post}, "
                f"ìˆ˜ì§‘ëŒ€ìƒê¸€ìˆ˜={len(tasks)}"
            )

            # 3) ì´ í˜ì´ì§€ì— from_date ì´ìƒì¸ ê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´
            #    ì•„ë˜ í˜ì´ì§€ëŠ” ì „ë¶€ ë” ì˜›ë‚  ê¸€ â†’ ë” ë³¼ í•„ìš” ì—†ìŒ
            if not page_has_not_too_old_post:
                print(f"[ì¤‘ë‹¨] {current_url} ì´í›„ë¡œëŠ” {from_date} ì´ì „ ê¸€ë§Œ ìˆìŒ â†’ ì¢…ë£Œ")
                break

            # 4) ìƒì„¸ í˜ì´ì§€ ë³‘ë ¬ ìš”ì²­
            if tasks:
                max_workers = getattr(self, "max_workers", 5)

                with ThreadPoolExecutor(max_workers=max_workers) as ex:
                    futures = [
                        ex.submit(self._fetch_detail, post_url, title, post_date)
                        for (post_url, title, post_date, post_no) in tasks
                    ]

                    for (future, (post_url, title, post_date, post_no)) in zip(futures, tasks):
                        try:
                            item = future.result()
                        except Exception as e:
                            print(f"[ì—ëŸ¬] ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {post_url}, error={e}")
                            continue

                        if item is None:
                            continue

                        item["post_no"] = post_no
                        # board_base_urlì€ ë°”ê¹¥ì—ì„œ ë¶™ì´ë‹ˆê¹Œ ì—¬ê¸°ì„  ì•ˆ ê±´ë“œë¦¼
                        yield item

            # 5) ë‹¤ìŒ í˜ì´ì§€ ì´ë™
            time.sleep(0.1)  # ì„œë²„ ì˜ˆì˜ìƒ ì ê¹ ì‰¼

            paging_wrap = soup.find("div", class_="b-paging-wrap")
            if not paging_wrap:
                print("[ì¤‘ë‹¨] b-paging-wrap ì—†ìŒ â†’ ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ íŒë‹¨")
                break

            next_a = paging_wrap.select_one("li.next.pager > a")
            if not next_a:
                print("[ì¤‘ë‹¨] li.next.pager a ì—†ìŒ â†’ ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ")
                break

            href = next_a.get("href")
            if not href or href.startswith("javascript"):
                print(f"[ì¤‘ë‹¨] next ë§í¬ ì´ìƒí•¨: href={href}")
                break

            next_url = urljoin(base_url, href)
            if next_url in visited:
                print(f"[ì¤‘ë‹¨] ë‹¤ìŒ URLì´ ì´ë¯¸ ë°©ë¬¸í•œ URL: {next_url}")
                break

            print(f"ë‹¤ìŒ í˜ì´ì§€ ì´ë™: {next_url}")
            current_url = next_url



    def crawl_academic_board(
        self,
        save_path="academic_board.jsonl",
        chunk_size=100,
        days_per_step=100,   # â˜… 100ì¼ ë‹¨ìœ„
        total_days=730,      # â˜… ì „ì²´ëŠ” 2ë…„ì¹˜ ì •ë„
    ):
        from datetime import datetime, timedelta
        from pathlib import Path
        import json

        board_urls = [
            "https://www.hongik.ac.kr/kr/newscenter/notice.do",
        ]

        save_path = Path(save_path)

        # === chunk ìƒíƒœ ===
        current_chunk_idx = 0
        current_items = []
        current_min_date = None
        current_max_date = None

        def flush_current_chunk():
            nonlocal current_chunk_idx, current_items, current_min_date, current_max_date

            if not current_items:
                return

            if current_min_date is None or current_max_date is None:
                chunk_label = f"idx={current_chunk_idx}"
                min_date_str = None
                max_date_str = None
            else:
                chunk_label = f"{current_max_date:%Y.%m.%d}~{current_min_date:%Y.%m.%d}"
                min_date_str = current_min_date.isoformat()
                max_date_str = current_max_date.isoformat()

            record = {
                "chunk_meta": {
                    "idx": current_chunk_idx,
                    "label": chunk_label,
                    "count": len(current_items),
                    "min_date": min_date_str,
                    "max_date": max_date_str,
                },
                "items": current_items,
            }

            with save_path.open("a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")

            print(
                f"[ì €ì¥] idx={current_chunk_idx}, "
                f"ê¸°ê°„={chunk_label}, ê°œìˆ˜={len(current_items)} â†’ {save_path}"
            )

            current_items = []
            current_min_date = None
            current_max_date = None
            current_chunk_idx += 1

        # === ë‚ ì§œ ë²”ìœ„ë¥¼ 100ì¼ì”© ìë¥´ê¸° ===
        today = datetime.now().date()
        oldest = today - timedelta(days=total_days)

        # ex) [ì˜¤ëŠ˜-0~99], [100~199], ... ì´ëŸ° ì‹ìœ¼ë¡œ ë’¤ë¡œ ë‚´ë ¤ê°€ë©´ì„œ
        date_ranges = []
        cur_end = today
        while cur_end > oldest:
            cur_start = max(oldest, cur_end - timedelta(days=days_per_step - 1))
            # ê²¹ì¹˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•´ ë‹¤ìŒ êµ¬ê°„ end = start - 1
            date_ranges.append((cur_start, cur_end))
            cur_end = cur_start - timedelta(days=1)

        # ìµœì‹  êµ¬ê°„ë¶€í„° ëŒê³  ì‹¶ìœ¼ë©´ ê·¸ëƒ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # ì˜¤ë˜ëœ ê²ƒë¶€í„° ëŒê³  ì‹¶ìœ¼ë©´ date_ranges.reverse()

        for (from_date, to_date) in date_ranges:
            print(f"\n[ë²”ìœ„ ì‹œì‘] {from_date} ~ {to_date}")

            for base_url in board_urls:
                print(f"[ì‹œì‘] {base_url} / {from_date}~{to_date}")

                for item in self._crawl_single_board(base_url, from_date, to_date):
                    item["board_base_url"] = base_url

                    post_date = datetime.strptime(item["date"], "%Y.%m.%d").date()
                    if current_min_date is None or post_date < current_min_date:
                        current_min_date = post_date
                    if current_max_date is None or post_date > current_max_date:
                        current_max_date = post_date

                    current_items.append(item)

                    if len(current_items) >= chunk_size:
                        flush_current_chunk()

                print(f"[ì™„ë£Œ] {base_url} / {from_date}~{to_date}")

        flush_current_chunk()
        print("[ì „ì²´ ì™„ë£Œ]")



    # ---------------- 3. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ ê°œì„¤ê³¼ëª© ---------------- #

    def crawl_ie_courses(self):
        url = "https://ie.hongik.ac.kr/ie/0301.do"

        options = Options()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        driver = webdriver.Chrome(options=options)
        driver.get(url)

        try:
            ul_grid = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "ul.grid"))
            )

            boxes = ul_grid.find_elements(By.CSS_SELECTOR, "div.curriculum-title-box")

            courses = []
            for idx, box in enumerate(boxes, start=1):
                # ğŸ”¹ HTML íƒœê·¸ X, í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
                text = box.text.strip()

                courses.append({
                    "index": idx,
                    "text": text
                })

            return courses

        finally:
            driver.quit()



    # ---------------- 4. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ í•™ê³¼ ê³µì§€ì‚¬í•­ ---------------- #

    def crawl_ie_board(self):
        """
        ì‚°ì—…ë°ì´í„°ê³µí•™ê³¼(ë° ë™ì¼ í…œí”Œë¦¿ í•™ê³¼)ì˜ í•™ê³¼ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§
        - board_urls ë¦¬ìŠ¤íŠ¸ì˜ ê° URLì„ 1í˜ì´ì§€ë¶€í„° ì‹œì‘
        - ê° URL(í•™ë¶€/í•™ê³¼)ë§ˆë‹¤:
          1) 1í˜ì´ì§€ë¶€í„° ì‹œì‘
          2) ê° í˜ì´ì§€ì—ì„œ ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ë§Œ ìƒì„¸ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§
          3) í•´ë‹¹ í˜ì´ì§€ì— ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ -> ê·¸ í•™ë¶€ í¬ë¡¤ë§ ì¢…ë£Œ
          4) div.b-paging ì•ˆì—ì„œ (í˜„ì¬í˜ì´ì§€+1) í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ a íƒœê·¸ë¥¼ ì°¾ì•„ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        """

        # âœ… ì—¬ê¸°ì„œ name, urlì„ ê°™ì´ ê´€ë¦¬
        boards = [
            {"name": "ì‚°ì—…ë°ì´í„°ê³µí•™ê³¼", "url": "https://ie.hongik.ac.kr/ie/0401.do"},
            
            # ê³µê³¼ëŒ€í•™_(ì‹ ì†Œì¬ê³µí•™ì „ê³µ ë§í¬ ì ‘ê·¼ ë¶ˆê°€, ê¸°ì´ˆê³¼í•™ê³¼ ê¸€ ì—†ìŒ)
            {"name": "ì „ê¸°ì „ìê³µí•™ë¶€", "url": "https://ee.hongik.ac.kr/ee/0501.do"},
            {"name": "í™”í•™ê³µí•™ì „ê³µ", "url": "https://chemeng.hongik.ac.kr/chemeng/sub/0401.do"},
            {"name": "ì»´í“¨í„°ê³µí•™ê³¼", "url": "https://wwwce.hongik.ac.kr/wwwce/0401.do"},
            {"name": "ê¸°ê³„ì‹œìŠ¤í…œë””ìì¸ê³µí•™ê³¼", "url": "https://me.hongik.ac.kr/me/0701.do"},
            {"name": "ê±´ì„¤í™˜ê²½ê³µí•™ê³¼", "url": "https://civil.hongik.ac.kr/civil/0401.do"},

            # ê²½ì˜ëŒ€í•™
            {"name": "ê²½ì˜ëŒ€í•™", "url": "https://bizadmin.hongik.ac.kr/bizadmin/0401.do"},

            # ë²•ê³¼ëŒ€í•™
            {"name": "ë²•ê³¼ëŒ€í•™", "url": "https://law.hongik.ac.kr/law/0401.do"},

            # ë¯¸ìˆ ëŒ€í•™_(ì‹œê°ë””ìì¸/ê¸ˆì†ì¡°í˜•ë””ìì¸ ë‹¤ë¥¸ í˜•ì‹ì˜ í™ˆí˜ì´ì§€)
            {"name": "ë™ì–‘í™”ê³¼", "url": "https://orip.hongik.ac.kr/orip/0401.do"},
            {"name": "íšŒí™”ê³¼", "url": "https://painting.hongik.ac.kr/painting/0401.do"},
            {"name": "íŒí™”ê³¼", "url": "https://printmk.hongik.ac.kr/printmk/0401.do"},
            {"name": "ì¡°ì†Œê³¼", "url": "https://scu.hongik.ac.kr/scu/0401.do"},
            {"name": "ì‚°ì—…ë””ìì¸ì „ê³µ", "url": "https://id.hongik.ac.kr/id/0401.do"},
            {"name": "ë„ì˜ˆìœ ë¦¬ê³¼", "url": "https://cer.hongik.ac.kr/cer/0401.do"},
            {"name": "ëª©ì¡°í˜•ê°€êµ¬í•™ê³¼", "url": "https://waf.hongik.ac.kr/waf/0401.do"},
            {"name": "ì˜ˆìˆ í•™ê³¼", "url": "https://art.hongik.ac.kr/art/0401.do"},

            # ë””ìì¸ì˜ˆìˆ ê²½ì˜í•™ë¶€
            {"name": "ë””ìì¸ì˜ˆìˆ ê²½ì˜í•™ë¶€", "url": "https://iim.hongik.ac.kr/iim/0401.do"},

            # ìº í¼ìŠ¤ììœ¨ì „ê³µ(ì„œìš¸)
            {"name": "ìº í¼ìŠ¤ììœ¨ì „ê³µ", "url": "https://fm.hongik.ac.kr/fm/0401.do"},

            # ë°”ì´ì˜¤í—¬ìŠ¤ìœµí•©í•™ë¶€
            {"name": "ë°”ì´ì˜¤í—¬ìŠ¤ìœµí•©í•™ë¶€", "url": "https://biocoss.hongik.ac.kr/biocoss/0401.do"},

            # ê³¼í•™ê¸°ìˆ ëŒ€í•™
            {"name": "ê³¼í•™ê¸°ìˆ ëŒ€í•™", "url": "https://cst.hongik.ac.kr/cst/0501.do"},

            # ê±´ì¶•ë„ì‹œëŒ€í•™_(ê±´ì¶•ê³µí•™/ë„ì‹œê³µí•™ê³¼ íŒ¨ìŠ¤ ë‹¤ë¥¸ í˜•ì‹ì˜ í™ˆí˜ì´ì§€)

            # # ë¬¸ê³¼ëŒ€í•™
            {"name": "ì˜ì—¬ì˜ë¬¸í•™ê³¼", "url": "https://english.hongik.ac.kr/english/0401.do"},
            {"name": "ë…ì–´ë…ë¬¸í•™ê³¼", "url": "https://german.hongik.ac.kr/german/0401.do"},
            {"name": "ë¶ˆì–´ë¶ˆë¬¸í•™ê³¼", "url": "https://france.hongik.ac.kr/france/0401.do"},
            {"name": "êµ­ì–´êµ­ë¬¸í•™ê³¼", "url": "https://hkorean.hongik.ac.kr/hkorean/0401.do"},

            # ì‚¬ë²”ëŒ€í•™
            {"name": "ìˆ˜í•™êµìœ¡ê³¼", "url": "https://math.hongik.ac.kr/math/0401.do"},
            {"name": "êµ­ì–´êµìœ¡ê³¼", "url": "https://koredu.hongik.ac.kr/koredu/0401.do"},
            {"name": "ì˜ì–´êµìœ¡ê³¼", "url": "https://educomplex.hongik.ac.kr/educomplex/0401.do"},
            {"name": "ì—­ì‚¬êµìœ¡ê³¼", "url": "https://hisedu.hongik.ac.kr/hisedu/0401.do"},
            {"name": "êµìœ¡í•™ê³¼", "url": "https://edu.hongik.ac.kr/edu/0401.do"},

            # ê²½ì œí•™ë¶€
            {"name": "ê²½ì œí•™ë¶€", "url": "https://economics.hongik.ac.kr/economics/0401.do"},

            # # ê³µì—°ì˜ˆìˆ í•™ë¶€
            {"name": "ë®¤ì§€ì»¬ì „ê³µ", "url": "https://musical.hongik.ac.kr/musical/0501.do"},
            {"name": "ì‹¤ìš©ìŒì•…ì „ê³µ", "url": "https://music.hongik.ac.kr/music/0501.do"},

            # ìœµí•©ì „ê³µ 
            # ì•„ë˜ëŠ” í™ˆí˜ì´ì§€ ì—†ëŠ” í•™ê³¼ë“¤
            # ê³µì—°ì˜ˆìˆ ì „ê³µ/ê±´ì¶•ê³µê°„ì˜ˆìˆ ì „ê³µ/ì‚¬ë¬¼ì¸í„°ë„·ê³µí•™/ì§€ëŠ¥ë¡œë´‡ê³µí•™/ìŠ¤ë§ˆíŠ¸ë„ì‹œë°ì´í„°ì‚¬ì´ì–¸ìŠ¤
            # ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤/ì˜ë£Œí—¬ìŠ¤ì¼€ì–´AI/í—¬ìŠ¤ì¼€ì–´ì„œë¹„ìŠ¤ì „ê³µ
            {"name": "ë¬¸í™”ì˜ˆìˆ ê²½ì˜í•™ê³¼", "url": "https://hicam.hongik.ac.kr/hicam/0401.do"},
            {"name": "ë””ìì¸ì—”ì§€ë‹ˆì–´ë§ì „ê³µ", "url": "https://smpd.hongik.ac.kr/smpd/0401.do"},
            
        ]

        # six_months_ago = datetime.now() - timedelta(days=180)  # ì‹¤ì œ ìš´ì˜
        six_months_ago = datetime.now() - timedelta(days=730)      # 2ë…„ì¹˜ë¡œ ìš´ì˜ì¡°ì •

        results_by_board = {}

        for board in boards:
            name = board["name"]      # ë”•ì…”ë„ˆë¦¬ keyë¡œ ì“¸ ì´ë¦„
            base_url = board["url"]   # í¬ë¡¤ë§ì— ì‚¬ìš©í•  URL

            print(f"í•™ê³¼ {name} í¬ë¡¤ë§ ì‹œì‘...")
            board_results = self._crawl_single_ie_board(base_url, six_months_ago)

            # âœ… ì—¬ê¸°ì„œ URLì´ ì•„ë‹ˆë¼ nameì„ keyë¡œ ì‚¬ìš©
            results_by_board[name] = board_results

            # âœ… í¬ë¡¤ë§ ì§„í–‰ìƒí™© ì¶œë ¥ (ì›í•˜ëŠ” ë©˜íŠ¸ë¡œ ìˆ˜ì • ê°€ëŠ¥)
            print(f"[í¬ë¡¤ë§ ì™„ë£Œ] {name} ({base_url}) - {len(board_results)}ê±´ ìˆ˜ì§‘")

        return results_by_board



    def _crawl_single_ie_board(self, base_url, six_months_ago):
        """
        í•˜ë‚˜ì˜ ì‚°ì—…ë°ì´í„°ê³µí•™ê³¼(ë˜ëŠ” ë™ì¼ í…œí”Œë¦¿ í•™ê³¼) ê³µì§€ ê²Œì‹œíŒì— ëŒ€í•´
        'ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ë§Œ' í˜ì´ì§€ë¥¼ ë„˜ê¸°ë©° í¬ë¡¤ë§í•˜ëŠ” ê³µí†µ ë¡œì§
        """

        results = []
        current_page = 1
        current_url = base_url
        visited = set()

        while True:
            # í˜¹ì‹œ ì¤‘ë³µ ìš”ì²­ ë°©ì§€
            if current_url in visited:
                break
            visited.add(current_url)

            # --- í˜„ì¬ í˜ì´ì§€ ìš”ì²­ --- #
            try:
                resp = self.session.get(
                    current_url,
                    headers=self.headers,
                    timeout=10,
                    verify=False,   # ğŸ”¹ SSL ì¸ì¦ì„œ ê²€ì¦ ë”
                )
            except requests.exceptions.SSLError as e:
                print(f"[ê²½ê³ ] í•™ê³¼ ê³µì§€ ìš”ì²­ ì¤‘ SSL ì—ëŸ¬ ë°œìƒ, ie_board í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
                break

            if not resp.ok:
                break

            soup = BeautifulSoup(resp.text, "html.parser")

            # ê²Œì‹œë¬¼ ëª©ë¡ tr
            posts = soup.select("tbody tr") or soup.select("tr")

            # ì´ í˜ì´ì§€ì— 'ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€'ì´ ìˆì—ˆëŠ”ì§€
            page_has_recent_post = False

            for post in posts:
                # ì œëª© a íƒœê·¸ ì—†ìœ¼ë©´ ìŠ¤í‚µ (í—¤ë”/ë¹ˆ í–‰ ë“±)
                link_elem = post.find("a")
                if not link_elem:
                    continue

                # ê³µì§€(ìƒë‹¨ ê³ ì •) ì œì™¸í•˜ê³  ì‹¶ìœ¼ë©´: ì²« ë²ˆì§¸ tdê°€ 'ê³µì§€'ë©´ ìŠ¤í‚µ
                # tds = post.find_all("td")
                # if tds and tds[0].get_text(strip=True) == "ê³µì§€":
                #     continue

                post_date = None
                tds = post.find_all("td")

                for td in tds:
                    text = td.get_text(strip=True)
                    # YYYY.MM.DD í˜•íƒœì¸ì§€ ê²€ì‚¬
                    if re.fullmatch(r"\d{4}\.\d{2}\.\d{2}", text):
                        try:
                            post_date = datetime.strptime(text, "%Y.%m.%d")
                        except ValueError:
                            post_date = None
                        break

                # ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ìŠ¤í‚µ
                if not post_date:
                    continue

                # 6ê°œì›” ì´ë‚´ ê¸€ì¸ì§€ í™•ì¸
                if post_date >= six_months_ago:
                    page_has_recent_post = True
                else:
                    # ì´ ê¸€ì€ ë„ˆë¬´ ì˜¤ë˜ëœ ê¸€ì´ë¼ ìƒì„¸ í¬ë¡¤ë§ ì•ˆ í•¨
                    continue

                title = link_elem.get_text(strip=True)
                href = link_elem.get("href")
                if not href:
                    continue

                post_url = urljoin(current_url, href)

                # --- ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ìš”ì²­ --- #
                try:
                    detail_resp = self.session.get(post_url, headers=self.headers)
                    if not detail_resp.ok:
                        continue
                except Exception:
                    continue

                detail_soup = BeautifulSoup(detail_resp.text, "html.parser")

                # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹ ê·¸ëŒ€ë¡œ ìœ ì§€)
                content = {
                    "url": post_url,
                    "title": "",
                    "content": "",
                    "date": post_date.strftime("%Y.%m.%d"),
                    "attachments": [],
                }

                # ì œëª© ë° ë³¸ë¬¸
                title_elem = detail_soup.select_one(".view_title") or detail_soup.find("h4")
                if title_elem:
                    content["title"] = title_elem.get_text(strip=True)
                else:
                    content["title"] = title

                body_elem = detail_soup.select_one(".view_content") or detail_soup.find("div", class_="view_con")
                if body_elem:
                    content["content"] = body_elem.get_text(strip=True)
                else:
                    # fallback: í˜ì´ì§€ ì „ì²´ì—ì„œ ë³¸ë¬¸ í›„ë³´ ì˜ì—­ì„ ëª» ì°¾ìœ¼ë©´ ê·¸ëƒ¥ ì „ì²´ í…ìŠ¤íŠ¸ ì¼ë¶€
                    content["content"] = detail_soup.get_text(separator="\n", strip=True)

                # ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
                attachments = detail_soup.select(".file_download a")
                for attachment in attachments:
                    file_url = urljoin(post_url, attachment.get("href", ""))
                    file_name = attachment.get_text(strip=True)

                    file_info = {"name": file_name, "content": None}

                    # # PDFë©´ ë‚´ìš© ì¶”ì¶œ ì‹œë„
                    # if file_name.lower().endswith(".pdf"):
                    #     try:
                    #         file_resp = self.session.get(file_url, headers=self.headers)
                    #         if file_resp.ok:
                    #             pdf_text = self.extract_pdf_text(file_resp.content)
                    #             file_info["content"] = pdf_text
                    #     except Exception:
                    #         file_info["content"] = "PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"

                    # content["attachments"].append(file_info)

                results.append(content)
                time.sleep(0.2)

            # âœ… ì´ í˜ì´ì§€ì— ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë” ì´ìƒ ë‚´ë ¤ê°ˆ í•„ìš” ì—†ìŒ
            if not page_has_recent_post:
                break

            # --- ë‹¤ìŒ í˜ì´ì§€: (í˜„ì¬í˜ì´ì§€ + 1) í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ a íƒœê·¸ ì°¾ê¸° --- #
            paging_div = soup.find("div", class_="b-paging")
            if not paging_div:
                break

            next_page_num = current_page + 1
            next_link_tag = None

            for a in paging_div.find_all("a"):
                text = a.get_text(strip=True)
                if text == str(next_page_num):  # "2", "3", ...
                    next_link_tag = a
                    break

            # ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not next_link_tag:
                break

            href = next_link_tag.get("href")
            if not href or href.startswith("javascript"):
                break

            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            current_url = urljoin(base_url, href)
            current_page = next_page_num
            time.sleep(0.2)

        return results



    # ---------------- ê²°ê³¼ ì €ì¥ & ì‹¤í–‰ ---------------- #

    def save_results(self, data, filename):
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def run(self, user_id=None, password=None):
        print("í¬ë¡¤ë§ ì‹œì‘...")

        all_results = {}

        # 1. CN í™ìµ ë¡œê·¸ì¸ (í•„ìš”í•  ë•Œë§Œ)
        if user_id and password:
            print("1. CN í™ìµ ë¡œê·¸ì¸ ì‹œë„...")
            if self.login_cn_hongik(user_id, password):
                print("   ë¡œê·¸ì¸ ì„±ê³µ")
            else:
                print("   ë¡œê·¸ì¸ ì‹¤íŒ¨ (login_cn_hongik ë‚´ìš© ìˆ˜ì • í•„ìš”)")

        # 2. í•™ì‚¬ ê³µì§€ì‚¬í•­
        # print("2. í•™ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§...")
        # academic_data = self.crawl_academic_board() or []   # âœ… Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
        # all_results['academic_board'] = academic_data
        # print(f"   {len(academic_data)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")

        # 3. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ ê°œì„¤ê³¼ëª©
        print("3. ê°œì„¤ê³¼ëª© í¬ë¡¤ë§...")
        courses_data = self.crawl_ie_courses()
        all_results['ie_courses'] = courses_data
        print(f"   {len(courses_data)}ê°œ ê³¼ëª© í¬ë¡¤ë§ ì™„ë£Œ")

        # 4. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ í•™ê³¼ ê³µì§€ì‚¬í•­
        print("4. í•™ê³¼ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§...")
        ie_board_data = self.crawl_ie_board()
        all_results["ie_board"] = ie_board_data
        print(f"   {len(ie_board_data)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")

        # ê²°ê³¼ ì €ì¥
        self.save_results(all_results, "hongik_crawled_data.json")
        print("\ní¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ê°€ 'hongik_crawled_data.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        return all_results


if __name__ == "__main__":
    crawler = HongikCrawler()
    courses = crawler.crawl_ie_courses()
    print(len(courses))
    print(courses[:3])

    # âš ï¸ ì¤‘ìš”í•œ ë³´ì•ˆ ì£¼ì˜:
    #   ì‹¤ì œ ì½”ë“œì—ëŠ” í•™ë²ˆ/ë¹„ë°€ë²ˆí˜¸ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ë§ê³ 
    #   í™˜ê²½ë³€ìˆ˜ë‚˜ ë³„ë„ ì„¤ì • íŒŒì¼ì—ì„œ ì½ì–´ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê±¸ ì¶”ì²œ.
    #
    # ì˜ˆì‹œ:
    # import os
    # user = os.environ.get("HONGIK_ID")
    # pw = os.environ.get("HONGIK_PW")
    # results = crawler.run(user_id=user, password=pw)

    # ë¡œê·¸ì¸ ì—†ì´ ê³µê°œ í˜ì´ì§€ë§Œ í¬ë¡¤ë§:
    results = crawler.run()
