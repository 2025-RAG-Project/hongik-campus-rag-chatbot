import pandas as pd
import os
import shutil
import re
import ast
import pickle # íŒŒì´ì¬ ê°ì²´ ì••ì¶•ìš©
from dotenv import load_dotenv

from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# PDR ê´€ë ¨
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import LocalFileStore, EncoderBackedStore

load_dotenv()

CSV_PATH = "build_vector_db/data/df_json_to_csv.csv"
CHROMA_DIR = "build_vector_db/chroma_db" # ë²¡í„°(ê²€ìƒ‰ìš©) ì €ì¥ê²½ë¡œ
DOCSTORE_DIR = "build_vector_db/docstore" # ì›ë³¸(ì°¸ì¡°ìš©) ì €ì¥ê²½ë¡œ
COLLECTION_NAME = "hongik_data"


# ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_text(t: str):
    if pd.isna(t): return "" # NaN ì²˜ë¦¬ ì¶”ê°€
    t = str(t)
    t = re.sub(r"<[^>]+>", " ", t)
    t = t.replace("\n", " ").replace("\r", " ").replace("\t", " ")
    t = " ".join(t.split())
    return t

def normalize_date(date_str: str):
    if pd.isna(date_str): return "ë‚ ì§œë¯¸ìƒ"
    date_str = str(date_str)
    date_str = date_str.replace(".", "-")
    return date_str

# Chroma DB êµ¬ì¶• í•¨ìˆ˜
def build_chroma_db():

    # 1. ê¸°ì¡´ DB ì‚­ì œ
    if os.path.exists(CHROMA_DIR): shutil.rmtree(CHROMA_DIR)
    if os.path.exists(DOCSTORE_DIR): shutil.rmtree(DOCSTORE_DIR)

    df = pd.read_csv(CSV_PATH)
    df = df.dropna(subset=["content"]).reset_index(drop=True)

    # 2. Splitter ì„¤ì •

    # [Child] ê²€ìƒ‰ìš© ì‘ì€ ì¡°ê°
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=50,
        separators=["\n\n", "\n", " ", ""] # ë¬¸ë‹¨ -> ì¤„ -> ë‹¨ì–´ ìˆœìœ¼ë¡œ split
    )

    # [Parent] ì›ë³¸ ì €ì¥ìš©
    parent_splitter = None # ê²Œì‹œê¸€ í•˜ë‚˜ë¥¼ í†µì§¸ë¡œ ì“°ê¸° ìœ„í•´

    # 3. ì €ì¥ì†Œ ì„¤ì •
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    # [Vector Store] ìì‹(ë²¡í„°) ì¡°ê° ì €ì¥ (Chroma)
    vectorstore = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_DIR
    )
    # [Doc Store] ë¶€ëª¨(ì›ë³¸) ì €ì¥ (LocalFileStore)
    fs = LocalFileStore(DOCSTORE_DIR)
    docstore = EncoderBackedStore(
        store=fs,
        key_encoder=lambda x: x,
        value_serializer=pickle.dumps,
        value_deserializer=pickle.loads
    )
    
    # 4. PDR ìƒì„±
    retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=docstore,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter
    )

    # 5. ë¬¸ì„œ ê°ì²´ ìƒì„± (ì „ì²˜ë¦¬ ë° ë©”íƒ€ë°ì´í„°)
    parent_docs = []
    for _, row in df.iterrows():
        title = clean_text(row["title"])
        raw_content = clean_text(row["content"])
        department = str(row["department"])
        
        # attachment íŒŒì‹±
        has_attachment = False
        attachment_names = []
        raw_attachments = str(row["attachments"])
        if not pd.isna(raw_attachments):
            try:
                parsed_data = ast.literal_eval(str(raw_attachments))
                if isinstance(parsed_data, (dict,tuple)):
                    parsed_data = [parsed_data] if isinstance(parsed_data,dict) else list(parsed_data)

                seen_names = set()
                for item in parsed_data:
                    if isinstance(item, dict) and 'name' in item:
                        name = item['name']
                        if name not in seen_names:
                            attachment_names.append(name)
                            seen_names.add(name)
                if attachment_names:
                    has_attachment = True
            except (ValueError, SyntaxError):
                if isinstance(raw_attachments, str) and len(raw_attachments) > 5:
                    attachment_names.append(raw_attachments[:50] + "...")
                    has_attachment = True
        attachment_name_str = ", ".join(attachment_names) if attachment_names else "ì—†ìŒ"    

        # indexì¹¼ëŸ¼ì—ì„œ notice_type ì¶”ì¶œí•˜ê¸°
        raw_index = str(row["index"]) # 'univ_notice_100'
        if "_" in raw_index:
            notice_type_code = raw_index.rsplit("_", 1)[0]
        else:
            notice_type_code = "general"
        type_mapping = {
            "course": "êµê³¼ëª©/ìˆ˜ê°•",
            "notice" : "í•™ê³¼ê³µì§€",
            "univ_notice": "ëŒ€í•™ê³µì§€"
        }
        notice_type_kr = type_mapping.get(notice_type_code, notice_type_code)

        # Date ì²˜ë¦¬
        final_date = "ë‚ ì§œë¯¸ìƒ"
        course_id = "í•´ë‹¹ì—†ìŒ"
        if notice_type_code == "course":
            course_id = str(row["date"]).strip()
            final_date = "ìƒì‹œ"
        else:
            final_date = normalize_date(row["date"])
            course_id = "í•´ë‹¹ì—†ìŒ"
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "title": title, # ê²Œì‹œê¸€ ì œëª©
            "url": row["url"], # ì›ë³¸ ë§í¬
            "date": final_date, # ë‚ ì§œ
            "course_id": course_id, # êµê³¼ëª© - í•™ìˆ˜ë²ˆí˜¸
            "department": department, # í•™ê³¼ëª…
            "notice_type": notice_type_kr, # ê³µì§€êµ¬ë¶„ (ëŒ€í•™ê³µì§€, í•™ê³¼ê³µì§€, êµê³¼ëª©/ìˆ˜ê°•)
            "has_attachment": has_attachment, # ì²¨ë¶€íŒŒì¼ ìœ ë¬´
            "attachment_name_str": attachment_name_str[:200], # íŒŒì¼ëª… ëª©ë¡
            "original_id": raw_index # [ê´€ë¦¬ìš©] ì›ë³¸ ê²Œì‹œê¸€ id
        }
        
        doc = Document(page_content=raw_content, metadata=metadata)
        parent_docs.append(doc)
    

    print(f"ì²˜ë¦¬í•  ì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(parent_docs)}")
    print("PDR ì¸ë±ì‹± ì²˜ë¦¬ì¤‘ (ìë™ìœ¼ë¡œ ìì‹ ìª¼ê°œê¸° ë° ì €ì¥)...")

    # openai í† í°ìˆ˜ ì œí•œ ë•Œë¬¸ì— batch_sizeë¥¼ 100ìœ¼ë¡œ ì„¤ì •
    batch_size = 100 
    
    try:
        from tqdm import tqdm
        iterator = tqdm(range(0, len(parent_docs), batch_size), desc="Indexing")
    except ImportError:
        iterator = range(0, len(parent_docs), batch_size)

    for i in iterator:
        batch = parent_docs[i : i + batch_size]
        try:
            retriever.add_documents(batch, ids=None)
            # tqdmì´ ì—†ì„ ë•Œë§Œ ë¡œê·¸ ì¶œë ¥
            if not isinstance(iterator, tqdm): 
                print(f"   - {i} ~ {i+len(batch)} ë²ˆì§¸ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ {i}ë²ˆì§¸ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    print("âœ… PDR êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“‚ ë²¡í„°DB ìœ„ì¹˜: {CHROMA_DIR}")
    print(f"ğŸ“‚ ë¬¸ì„œì €ì¥ì†Œ ìœ„ì¹˜: {DOCSTORE_DIR}")

if __name__ == "__main__":
    build_chroma_db()