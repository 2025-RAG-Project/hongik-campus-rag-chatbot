"""
í™ìµëŒ€í•™êµ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ì½”ë“œ (ìµœì†Œ ì˜ì¡´ì„± ë²„ì „)
- 2ë²ˆ: í•™ì‚¬ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒ í¬ë¡¤ë§ (ê° ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ê¹Œì§€ ë“¤ì–´ê°€ì„œ ë‚´ìš©/ì²¨ë¶€ ì²˜ë¦¬)
- 3ë²ˆ: ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ ê°œì„¤ê³¼ëª© (JS ë¡œë”© â†’ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì•Œì•„ì•¼ í•´ì„œ TODOë¡œ ì²˜ë¦¬)
- 4ë²ˆ: ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ í•™ê³¼ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (ê° ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ê¹Œì§€)
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import time
from urllib.parse import urljoin, urlparse, parse_qs
import PyPDF2
from io import BytesIO
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC



class HongikCrawler:
    DATE_PATTERN = re.compile(r"\d{4}\.\d{2}\.\d{2}")
    ATTACH_EXTS = (".pdf", ".hwp", ".hwpx", ".doc", ".docx",
                   ".xls", ".xlsx", ".ppt", ".pptx", ".zip")

    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko)"
            )
        }

    # ---------------- ê³µí†µ ìœ í‹¸ ---------------- #

    def _extract_date_from_row(self, tr):
        """tr ì•ˆì˜ td í…ìŠ¤íŠ¸ ì¤‘ 'YYYY.MM.DD' í˜•ì‹ì„ ì°¾ì•„ datetimeìœ¼ë¡œ ë°˜í™˜"""
        for td in tr.find_all("td"):
            text = td.get_text(strip=True)
            m = self.DATE_PATTERN.search(text)
            if m:
                try:
                    return datetime.strptime(m.group(), "%Y.%m.%d")
                except ValueError:
                    return None
        return None

    def _extract_article_text(self, soup, title=None):
        """
        ê²Œì‹œë¬¼ ìƒì„¸ í˜ì´ì§€ì—ì„œ íƒ€ì´í‹€/ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        - title: ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ì œëª©(ìˆìœ¼ë©´ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë³¸ë¬¸ êµ¬ê°„ ì˜ë¼ëƒ„)
        - í´ë˜ìŠ¤ëª…(.view_title ë“±)ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¼ë‚´ëŠ” ë°©ì‹
        """
        full_text = soup.get_text("\n", strip=True)
        lines = [l.strip() for l in full_text.splitlines() if l.strip()]

        # ì œëª© ìœ„ì¹˜ ì°¾ê¸° (ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ì œëª©ì´ ì‹¤ì œ í˜ì´ì§€ì—ë„ ë™ì¼í•˜ê²Œ ë‚˜íƒ€ë‚¨)
        title_line = None
        idx_title = 0
        if title:
            for i, line in enumerate(lines):
                if line == title:
                    title_line = line
                    idx_title = i
                    break

        if not title_line:
            # íƒ€ì´í‹€ì„ ëª» ì°¾ìœ¼ë©´ ì²« ë²ˆì§¸ë¡œ 'ê³µì§€ì‚¬í•­' ì•„ë˜ ë‚˜ì˜¤ëŠ” ì¤„ì„ ì œëª©ì´ë¼ê³  ê°€ì •
            # (ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ì™„ì „ ì •í™•í•˜ì§„ ì•Šì§€ë§Œ ìµœì†Œ ë™ì‘ìš©)
            for i, line in enumerate(lines):
                if "ê³µì§€ì‚¬í•­" in line:
                    # ê·¸ ë‹¤ìŒ non-empty lineì„ ì œëª©ìœ¼ë¡œ
                    for j in range(i + 1, len(lines)):
                        if lines[j]:
                            title_line = lines[j]
                            idx_title = j
                            break
                    break

        if not title_line:
            # ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´ ê·¸ëƒ¥ ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ ì²˜ë¦¬
            title_line = lines[0] if lines else ""
            idx_title = 0

        # ë³¸ë¬¸ êµ¬ê°„: [ì œëª© ë‹¤ìŒ ì¤„ ~ 'ì´ì „ê¸€/ë‹¤ìŒê¸€/ëª©ë¡' ì „ê¹Œì§€]
        body_lines = []
        for line in lines[idx_title + 1 :]:
            if line in ("ì´ì „ê¸€", "ë‹¤ìŒê¸€", "ëª©ë¡"):
                break
            # ê³µìœ /í”„ë¦°í„°/ë©”ë‰´ ê°™ì€ ë©”íƒ€ í…ìŠ¤íŠ¸ ì œê±°
            if any(
                key in line
                for key in ("ì¹´ì¹´ì˜¤ ê³µìœ í•˜ê¸°", "í˜ì´ìŠ¤ë¶ ê³µìœ í•˜ê¸°", "URL ê³µìœ í•˜ê¸°", "í”„ë¦°í„°")
            ):
                continue
            # 'ì²¨ë¶€íŒŒì¼' ë¬¸êµ¬ ìì²´ëŠ” ì œì™¸ (íŒŒì¼ ë¦¬ìŠ¤íŠ¸ëŠ” ë”°ë¡œ ì²˜ë¦¬)
            if "ì²¨ë¶€íŒŒì¼" in line:
                continue
            body_lines.append(line)

        body_text = "\n".join(body_lines).strip()
        return title_line, body_text

    def _extract_attachments(self, soup, page_url):
        """
        ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        - ì´ë¦„ê³¼ URL, (PDFì¸ ê²½ìš° ë‚´ìš© í…ìŠ¤íŠ¸)ê¹Œì§€
        - .pdf ì™¸ í™•ì¥ìëŠ” URLë§Œ ì €ì¥ (hwp í•´ì„ì€ ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•´ì„œ ì—¬ê¸°ì„  ì œì™¸)
        """
        attachments = []
        for a in soup.find_all("a"):
            name = a.get_text(strip=True)
            if not name:
                continue
            lower = name.lower()
            if not lower.endswith(self.ATTACH_EXTS):
                continue

            href = a.get("href")
            if not href or href.startswith("javascript"):
                continue

            file_url = urljoin(page_url, href)
            attach = {"name": name, "url": file_url, "content": None}

            # PDFì¸ ê²½ìš°ë§Œ ë‚´ìš©ê¹Œì§€ ì¶”ì¶œ (PyPDF2 ì‚¬ìš©)
            if lower.endswith(".pdf"):
                try:
                    resp = self.session.get(file_url, headers=self.headers)
                    if resp.ok:
                        attach["content"] = self.extract_pdf_text(resp.content)
                except Exception:
                    attach["content"] = "PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"

            attachments.append(attach)

        return attachments

    def extract_pdf_text(self, pdf_bytes):
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            pdf_file = BytesIO(pdf_bytes)
            reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in reader.pages:
                # PyPDF2ì˜ extract_textëŠ” ë²„ì „ì— ë”°ë¼ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                text += page.extract_text() or ""
            return text.strip() or "PDFì— ì¶”ì¶œ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        except Exception:
            return "PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"

    # ---------------- 1. CN í™ìµ ë¡œê·¸ì¸ ---------------- #

    def login_cn_hongik(self, user_id, password):
        """
        1ë²ˆ ë§í¬ - CN í™ìµ ë¡œê·¸ì¸
        âš ï¸ ì£¼ì˜: ì´ í˜ì´ì§€ëŠ” ì‹¤ì œë¡œëŠ” SSO/JS ê¸°ë°˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.
        â†’ ë°˜ë“œì‹œ ë¸Œë¼ìš°ì € ê°œë°œìë„êµ¬(Network íƒ­)ë¡œ ì‹¤ì œ ë¡œê·¸ì¸ ìš”ì²­ URL/íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•´ì„œ
          ì•„ë˜ login_action_url / login_dataë¥¼ ìˆ˜ì •í•´ì•¼ í•œë‹¤.
        """
        login_page = "https://cn.hongik.ac.kr/stud/"

        # ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì† (ì¿ í‚¤ ì„¸íŒ… ìš©ë„)
        try:
            self.session.get(login_page, headers=self.headers, timeout=10)
        except Exception:
            return False

        # TODO: ê°œë°œìë„êµ¬ì—ì„œ ì‹¤ì œ ë¡œê·¸ì¸ ìš”ì²­ URL/í•„ë“œ í™•ì¸ í›„ ìˆ˜ì •
        login_action_url = login_page  # ì˜ˆì‹œ: "https://cn.hongik.ac.kr/stud/jsp/login/check_login.jsp"
        login_data = {
            "id": user_id,
            "pw": password,
            # ì‹¤ì œ í•„ë“œëª…ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        }

        try:
            resp = self.session.post(
                login_action_url, data=login_data, headers=self.headers, timeout=10
            )
            return resp.ok
        except Exception:
            return False

    # ---------------- 2. í•™ì‚¬ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒ ---------------- #

    def crawl_academic_board(self):
        """
        í•™ì‚¬ ê³µì§€ì‚¬í•­(ë‰´ìŠ¤ì„¼í„° ê³µì§€) í¬ë¡¤ë§
        - ì‹œì‘: https://www.hongik.ac.kr/kr/newscenter/notice.do (1í˜ì´ì§€ë¼ê³  ê°€ì •)
        - ë™ì‘:
          1) í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ê¸€ì„ ëŒë©´ì„œ
             - ì‘ì„±ì¼ì´ ìµœê·¼ 6ê°œì›” ì´ë‚´ì¸ ê¸€ë§Œ ìƒì„¸í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§
          2) ê·¸ í˜ì´ì§€ì— 'ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€'ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ -> ì—¬ê¸°ì„œ ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ
          3) ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ b-paging ì•ˆì—ì„œ (í˜„ì¬í˜ì´ì§€+1) í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ a íƒœê·¸ë¥¼ ì°¾ì•„
             ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•˜ê³  1ë²ˆë¶€í„° ë°˜ë³µ
        """
        base_url = "https://www.hongik.ac.kr/kr/newscenter/notice.do"
        # six_months_ago = datetime.now() - timedelta(days=180) # ì˜¤ë˜ê±¸ë¦¬ë‹ˆ testì‹œì—ëŠ” 3ë§Œ í•´ì„œ ëŒë¦´ê²ƒ
        six_months_ago = datetime.now() - timedelta(days=20)

        results = []
        current_page = 1
        current_url = base_url
        visited = set()

        while True:
            if current_url in visited:
                # í˜¹ì‹œë‚˜ ë£¨í”„ ë„ëŠ” ìƒí™© ë°©ì§€
                break
            visited.add(current_url)

            # --- í˜„ì¬ í˜ì´ì§€ ìš”ì²­ --- #
            resp = self.session.get(current_url, headers=self.headers)
            if not resp.ok:
                break

            soup = BeautifulSoup(resp.text, "html.parser")

            rows = soup.select("tbody tr") or soup.select("tr")

            # ì´ í˜ì´ì§€ì—ì„œ 'ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€'ì´ ìˆì—ˆëŠ”ì§€ í‘œì‹œ
            page_has_recent_post = False

            for tr in rows:
                link_elem = tr.find("a")
                if not link_elem:
                    continue

                # ì‘ì„±ì¼ ì¶”ì¶œ
                post_date = self._extract_date_from_row(tr)
                if not post_date:
                    continue

                # 6ê°œì›” ì´ë‚´ì¸ì§€ ì²´í¬
                if post_date >= six_months_ago:
                    page_has_recent_post = True
                else:
                    # ì´ ê¸€ì€ ë„ˆë¬´ ì˜¤ë˜ëœ ê¸€ì´ë¼ì„œ í¬ë¡¤ë§ì—ì„œ ì œì™¸
                    continue

                title = link_elem.get_text(strip=True)
                href = link_elem.get("href")
                if not href:
                    continue

                post_url = urljoin(current_url, href)

                # --- ìƒì„¸ í˜ì´ì§€ ìš”ì²­ --- #
                detail_resp = self.session.get(post_url, headers=self.headers)
                if not detail_resp.ok:
                    continue

                detail_soup = BeautifulSoup(detail_resp.text, "html.parser")

                real_title, body = self._extract_article_text(detail_soup, title)
                attachments = self._extract_attachments(detail_soup, post_url)

                results.append({
                    "url": post_url,
                    "title": real_title,
                    "content": body,
                    "date": post_date.strftime("%Y.%m.%d"),
                    "attachments": attachments
                })

                time.sleep(0.2)

            # âœ… ì´ í˜ì´ì§€ì— ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´,
            #   ì´ ì´í›„ í˜ì´ì§€ë“¤ë„ ë” ì˜¤ë˜ëœ ê¸€ì¼ ê°€ëŠ¥ì„±ì´ í¬ë¯€ë¡œ ì—¬ê¸°ì„œ ì¢…ë£Œ
            if not page_has_recent_post:
                break

            # --- ë‹¤ìŒ í˜ì´ì§€: (í˜„ì¬í˜ì´ì§€ + 1) í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ a íƒœê·¸ ì°¾ê¸° --- #
            paging_div = soup.find("div", class_="b-paging")
            if not paging_div:
                break

            next_page_num = current_page + 1
            next_link_tag = None

            for a in paging_div.find_all("a"):
                text = a.get_text(strip=True)
                if text == str(next_page_num):   # "2", "3", ...
                    next_link_tag = a
                    break

            # ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not next_link_tag:
                break

            href = next_link_tag.get("href")
            if not href or href.startswith("javascript"):
                break

            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            current_url = urljoin(base_url, href)
            current_page = next_page_num
            time.sleep(0.2)

        return results


    # ---------------- 3. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ ê°œì„¤ê³¼ëª© ---------------- #

    def crawl_ie_courses(self):
        url = "https://ie.hongik.ac.kr/ie/0301.do"

        options = Options()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

        driver = webdriver.Chrome(options=options)
        driver.get(url)

        try:
            ul_grid = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "ul.grid"))
            )

            boxes = ul_grid.find_elements(By.CSS_SELECTOR, "div.curriculum-title-box")

            courses = []
            for idx, box in enumerate(boxes, start=1):
                # ğŸ”¹ HTML íƒœê·¸ X, í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
                text = box.text.strip()

                courses.append({
                    "index": idx,
                    "text": text
                })

            return courses

        finally:
            driver.quit()



    # ---------------- 4. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ í•™ê³¼ ê³µì§€ì‚¬í•­ ---------------- #

    def crawl_ie_board(self):
        """
        4ë²ˆ ë§í¬ - ì‚°ì—…ë°ì´í„°ê³µí•™ê³¼ í•™ê³¼ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§
        - URL: https://ie.hongik.ac.kr/ie/0401.do
        - ë™ì‘:
          1) 1í˜ì´ì§€ë¶€í„° ì‹œì‘
          2) ê° í˜ì´ì§€ì—ì„œ ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ë§Œ ìƒì„¸ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§
          3) í•´ë‹¹ í˜ì´ì§€ì— ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ -> ì „ì²´ í¬ë¡¤ë§ ì¢…ë£Œ
          4) div.b-paging ì•ˆì—ì„œ (í˜„ì¬í˜ì´ì§€+1) í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ a íƒœê·¸ë¥¼ ì°¾ì•„ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        """
        base_url = "https://ie.hongik.ac.kr/ie/0401.do"
        # six_months_ago = datetime.now() - timedelta(days=180)
        six_months_ago = datetime.now() - timedelta(days=20)

        results = []
        current_page = 1
        current_url = base_url
        visited = set()

        while True:
            # í˜¹ì‹œ ì¤‘ë³µ ìš”ì²­ ë°©ì§€
            if current_url in visited:
                break
            visited.add(current_url)

            # --- í˜„ì¬ í˜ì´ì§€ ìš”ì²­ --- #
            try:
                resp = self.session.get(
                    current_url,
                    headers=self.headers,
                    timeout=10,
                    verify=False,   # ğŸ”¹ SSL ì¸ì¦ì„œ ê²€ì¦ ë”
                )
            except requests.exceptions.SSLError as e:
                print(f"[ê²½ê³ ] í•™ê³¼ ê³µì§€ ìš”ì²­ ì¤‘ SSL ì—ëŸ¬ ë°œìƒ, ie_board í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
                break


            if not resp.ok:
                break

            soup = BeautifulSoup(resp.text, "html.parser")

            # ê²Œì‹œë¬¼ ëª©ë¡ tr
            posts = soup.select("tbody tr") or soup.select("tr")

            # ì´ í˜ì´ì§€ì— 'ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€'ì´ ìˆì—ˆëŠ”ì§€
            page_has_recent_post = False

            for post in posts:
                # ì œëª© a íƒœê·¸ ì—†ìœ¼ë©´ ìŠ¤í‚µ (í—¤ë”/ë¹ˆ í–‰ ë“±)
                link_elem = post.find("a")
                if not link_elem:
                    continue

                # ê³µì§€(ìƒë‹¨ ê³ ì •) ì œì™¸í•˜ê³  ì‹¶ìœ¼ë©´: ì²« ë²ˆì§¸ tdê°€ 'ê³µì§€'ë©´ ìŠ¤í‚µ
                tds = post.find_all("td")
                if tds and tds[0].get_text(strip=True) == "ê³µì§€":
                    continue

                # ë‚ ì§œ ì»¬ëŸ¼: ê¸°ì¡´ ì½”ë“œì²˜ëŸ¼ 3ë²ˆì§¸ td ê¸°ì¤€
                date_elem = post.select_one("td:nth-child(3)")
                post_date = None
                if date_elem:
                    try:
                        post_date = datetime.strptime(
                            date_elem.get_text(strip=True), "%Y.%m.%d"
                        )
                    except Exception:
                        post_date = None

                # ë‚ ì§œ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                if not post_date:
                    continue

                # 6ê°œì›” ì´ë‚´ ê¸€ì¸ì§€ í™•ì¸
                if post_date >= six_months_ago:
                    page_has_recent_post = True
                else:
                    # ì´ ê¸€ì€ ë„ˆë¬´ ì˜¤ë˜ëœ ê¸€ì´ë¼ ìƒì„¸ í¬ë¡¤ë§ ì•ˆ í•¨
                    continue

                title = link_elem.get_text(strip=True)
                href = link_elem.get("href")
                if not href:
                    continue

                post_url = urljoin(current_url, href)

                # --- ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ìš”ì²­ --- #
                try:
                    detail_resp = self.session.get(post_url, headers=self.headers)
                    if not detail_resp.ok:
                        continue
                except Exception:
                    continue

                detail_soup = BeautifulSoup(detail_resp.text, "html.parser")

                # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹ ê·¸ëŒ€ë¡œ ìœ ì§€)
                content = {
                    "url": post_url,
                    "title": "",
                    "content": "",
                    "date": post_date.strftime("%Y.%m.%d"),
                    "attachments": [],
                }

                # ì œëª© ë° ë³¸ë¬¸ (í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥)
                title_elem = detail_soup.select_one(".view_title") or detail_soup.find("h4")
                if title_elem:
                    content["title"] = title_elem.get_text(strip=True)
                else:
                    content["title"] = title

                body_elem = detail_soup.select_one(".view_content") or detail_soup.find("div", class_="view_con")
                if body_elem:
                    content["content"] = body_elem.get_text(strip=True)
                else:
                    # fallback: í˜ì´ì§€ ì „ì²´ì—ì„œ ë³¸ë¬¸ í›„ë³´ ì˜ì—­ì„ ëª» ì°¾ìœ¼ë©´ ê·¸ëƒ¥ ì „ì²´ í…ìŠ¤íŠ¸ ì¼ë¶€
                    content["content"] = detail_soup.get_text(separator="\n", strip=True)

                # ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œ ìŠ¤íƒ€ì¼ ìœ ì§€)
                attachments = detail_soup.select(".file_download a")
                for attachment in attachments:
                    file_url = urljoin(post_url, attachment.get("href", ""))
                    file_name = attachment.get_text(strip=True)

                    file_info = {"name": file_name, "content": None}

                    # PDFë©´ ë‚´ìš© ì¶”ì¶œ ì‹œë„
                    if file_name.lower().endswith(".pdf"):
                        try:
                            file_resp = self.session.get(file_url, headers=self.headers)
                            if file_resp.ok:
                                pdf_text = self.extract_pdf_text(file_resp.content)
                                file_info["content"] = pdf_text
                        except Exception:
                            file_info["content"] = "PDF ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"

                    content["attachments"].append(file_info)

                results.append(content)
                time.sleep(0.2)

            # âœ… ì´ í˜ì´ì§€ì— ìµœê·¼ 6ê°œì›” ì´ë‚´ ê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë” ì´ìƒ ë‚´ë ¤ê°ˆ í•„ìš” ì—†ìŒ
            if not page_has_recent_post:
                break

            # --- ë‹¤ìŒ í˜ì´ì§€: (í˜„ì¬í˜ì´ì§€ + 1) í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ a íƒœê·¸ ì°¾ê¸° --- #
            paging_div = soup.find("div", class_="b-paging")
            if not paging_div:
                break

            next_page_num = current_page + 1
            next_link_tag = None

            for a in paging_div.find_all("a"):
                text = a.get_text(strip=True)
                if text == str(next_page_num):  # "2", "3", ...
                    next_link_tag = a
                    break

            # ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if not next_link_tag:
                break

            href = next_link_tag.get("href")
            if not href or href.startswith("javascript"):
                break

            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            current_url = urljoin(base_url, href)
            current_page = next_page_num
            time.sleep(0.2)

        return results



    # ---------------- ê²°ê³¼ ì €ì¥ & ì‹¤í–‰ ---------------- #

    def save_results(self, data, filename):
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def run(self, user_id=None, password=None):
        print("í¬ë¡¤ë§ ì‹œì‘...")

        all_results = {}

        # 1. CN í™ìµ ë¡œê·¸ì¸ (í•„ìš”í•  ë•Œë§Œ)
        if user_id and password:
            print("1. CN í™ìµ ë¡œê·¸ì¸ ì‹œë„...")
            if self.login_cn_hongik(user_id, password):
                print("   ë¡œê·¸ì¸ ì„±ê³µ")
            else:
                print("   ë¡œê·¸ì¸ ì‹¤íŒ¨ (login_cn_hongik ë‚´ìš© ìˆ˜ì • í•„ìš”)")

        # 2. í•™ì‚¬ ê³µì§€ì‚¬í•­
        print("2. í•™ì‚¬ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§...")
        academic_data = self.crawl_academic_board() or []   # âœ… Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
        all_results['academic_board'] = academic_data
        print(f"   {len(academic_data)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")

        # 3. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ ê°œì„¤ê³¼ëª©
        print("3. ê°œì„¤ê³¼ëª© í¬ë¡¤ë§...")
        courses_data = self.crawl_ie_courses()
        all_results['ie_courses'] = courses_data
        print(f"   {len(courses_data)}ê°œ ê³¼ëª© í¬ë¡¤ë§ ì™„ë£Œ")

        # 4. ì‚°ì—…Â·ë°ì´í„°ê³µí•™ê³¼ í•™ê³¼ ê³µì§€ì‚¬í•­
        print("4. í•™ê³¼ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§...")
        ie_board_data = self.crawl_ie_board()
        all_results["ie_board"] = ie_board_data
        print(f"   {len(ie_board_data)}ê°œ ê²Œì‹œë¬¼ í¬ë¡¤ë§ ì™„ë£Œ")

        # ê²°ê³¼ ì €ì¥
        self.save_results(all_results, "hongik_crawled_data.json")
        print("\ní¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ê°€ 'hongik_crawled_data.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        return all_results


if __name__ == "__main__":
    crawler = HongikCrawler()
    courses = crawler.crawl_ie_courses()
    print(len(courses))
    print(courses[:3])

    # âš ï¸ ì¤‘ìš”í•œ ë³´ì•ˆ ì£¼ì˜:
    #   ì‹¤ì œ ì½”ë“œì—ëŠ” í•™ë²ˆ/ë¹„ë°€ë²ˆí˜¸ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ë§ê³ 
    #   í™˜ê²½ë³€ìˆ˜ë‚˜ ë³„ë„ ì„¤ì • íŒŒì¼ì—ì„œ ì½ì–´ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê±¸ ì¶”ì²œ.
    #
    # ì˜ˆì‹œ:
    # import os
    # user = os.environ.get("HONGIK_ID")
    # pw = os.environ.get("HONGIK_PW")
    # results = crawler.run(user_id=user, password=pw)

    # ë¡œê·¸ì¸ ì—†ì´ ê³µê°œ í˜ì´ì§€ë§Œ í¬ë¡¤ë§:
    results = crawler.run()
